
# Some shared parameters that can be referenced throughout the config.
human-radius: &human_radius 0.3
robot-radius: &robot_radius 0.3
time-step: &time_step 0.25


# Define the experiment's parameters.
training:
  # The number of learning nodes should be <= the GPU count unless you're
  # doing all CPU training.
  num-learners: 2
  # The number separate processes for exploring the environment _per learner node_.
  num-explorers: 2
  # The batch size on one training node (the same for all trainers)
  batch-size: 40

mu-zero:
  # Value and reward are scaled (with almost sqrt) and encoded on a vector with a range
  # of -support_size to support_size. Choose it so that
  # support_size <= sqrt(max(abs(discounted reward)))
  support-size: 10
  root-dirichlet-alpha: 0.25
  root-exploration-fraction: 0.25
  num-simulations: 50
  stacked-observations: 32
  pb-c-base: 19652
  pb-c-init: 1.25
  players: [0]
  # Chronological discount of the reward
  discount: 0.999
  # Number of steps in the future to take into account for calculating the target value
  td-steps: 30
  PER: True
  PER-alpha: 1.0
  num-unroll-steps: 10
  value-loss-weight: 0.25


# These are params sent to the crowd_sim environment
sim-environment:
  # How long to give robot to reach goal. If reached, this constitutes a timeout.
  time-limit: 30
  # The time step used to update pos + vel * (dt)
  time-step: *time_step
  # The number of humans in the simulation TODO(alex): What about a random amount?
  num-humans: 1
  
  world-width: 3
  world-height: 3
  # This could be the same as world size above, or if you want to confine the
  # goal location to a smaller region
  goal-location-width: 0.0
  goal-location-height: 0.0

# ORCA related configurations. NOTE, this is named for human planning, but
# if does take the robot(s) states into consideration.
human-motion-planner:
  # TODO(alex): Details on safety space
  safety-space: 0.0
  # The default maximum distance (center point to center point) to other agents a
  # new agent takes into account in the navigation. The larger this number, the
  # longer the running time of the simulation. If the number is too low, the
  # simulation will not be safe. Must be non-negative.
  neighbor-distance: 10.0
  # The default maximum number of other agents a new agent takes into account in the
  # navigation. The larger this number, the longer the running time of the simulation.
  # If the number is too low, the simulation will not be safe.
  max-neighbors: 10
  # The default minimal amount of time for which a new agent's velocities that are
  # computed by the simulation are safe with respect to other agents. The larger this
  # number, the sooner an agent will respond to the presence of other agents, but the
  # less freedom the agent has in choosing its velocities. Must be positive.
  time-horizon: 5.0
  # The default minimal amount of time for which a new agent's velocities that are
  # computed by the simulation are safe with respect to obstacles. The larger this
  # number, the sooner an agent will respond to the presence of obstacles, but the
  # less freedom the agent has in choosing its velocities. Must be positive.
  time-horizon-obst: 5.0
  #The default radius of a new agent. Must be non-negative.
  radius: *human_radius
  kinematics: "holonomic"
  # The default maximum speed of a new agent. Must be non-negative
  max-speed: 1.0
  # The time step of the simulation. Must be positive.
  time-step: *time_step


# The various rewards and penalities for different behavors. The penalties are
# based on social forces.
incentives:
  # Reaching goal is rewarded
  success: 1.0
  # Collision is penalized
  collision: -0.25
  # Keep the robot from coming to close to other agents.
  discomfort-distance: 0.2
  # TODO(alex): What is this?
  discomfort-penalty-factor: 0.5

# Define the parameters used to generate and control human agents.
human:
  # Use Optimal Reciprocal Collision Avoidance to model humans.
  policy: "orca"
  # Radius of circular region around human. Used to determine collisions.
  radius: *human_radius
  # The velocity human wants to accelerate to reach.
  preferred-velocity: 1.0

# Define the parameters used to generate and control the robot.
robot:
  # TODO(alex): Policy name?
  policy: None
  # Radius of circular region around robot. Used to determine collisions.
  radius: *robot_radius
  preferred-velocity: 1.0

# D-step planning used to predict value of potential states. These parameters
# define the space to sample for values. The total action space will be 
# speed-samples * rotation-samples + 1. The +1 is a standstill move.
action-space:
  # Define the kinematics equations used to update the position.
  kinematics: "holonomic"
  # The number of different speed samples to consider.
  speed-samples: 5
  # The number of rotation samples to consider.
  rotation-samples: 8

# Define the various models used during training.
models:
  # Social graph neural network.
  gnn-output-depth: &output-dim 32
  gnn: 
    # The dimension of the robot's state. The 9 values are:
    # [pos_x, pos_y, vel_x, vel_y, direction,
    # target_x, target_y, radius, preferred-velocity]
    robot-state-dimension: 2
    # The dimension of a human's observable state. The 5 values are:
    # pos_x, pos_y, vel_x, vel_y, direction.
    human-state-dimension: 5
    # The human and robot states are passed through a MLP to get equal output channels.
    # The last dimension here is used as the node channel depth in the graph network.
    mlp-layer-channels: [32, *output-dim]
    # The number of layers in the graph model.
    gnn-layers: 2
  
  state-net:
    time-step: *time_step
    channel-depth: *output-dim

  value-net:
    channel-depth: *output-dim
    net-dims: [*output-dim, *output-dim, *output-dim, 1]

# TODO(alex): Add optimizer type