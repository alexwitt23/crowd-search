
# Some shared parameters that can be referenced throughout the config.
human-radius: &human_radius 0.3
robot-radius: &robot_radius 0.3
time-step: &time_step 0.25


# Define the experiment's parameters.
training:
  num-epochs: 2000
  # The number of learning nodes should be <= the GPU count unless you're
  # doing all CPU training.
  num-learners: 2
  # The number separate processes for exploring the environment _per learner node_.
  num-explorers: 6
  # The batch size on one training node (the same for all trainers)
  batch-size: 2000
  # Set the maximum of history the storage node will hold. This sets a hard max limit
  # on memory usage:
  max-storage-memory: 10000
  # Whether or not to write gifs of the simulation environment
  visualize-results: false
  # Optimizer configuration
  optimizer:
    type: "Adam"  # CURRENTLY UNUSED
    # This is directly passed in as **kwargs to whatever optimizer is selected.
    kwargs:
      lr: 1.0e-3
      weight_decay: 1.0e-4


# These are params sent to the crowd_sim environment
sim-environment:
  # How long to give robot to reach goal. If reached, this constitutes a timeout.
  time-limit: 60
  # The time step used to update pos + vel * (dt)
  time-step: *time_step
  # The number of humans in the simulation TODO(alex): What about a random amount?
  num-humans: 1
  
  world-width: 6.0
  world-height: 6.0
  # This could be the same as world size above, or if you want to confine the
  # goal location to a smaller region
  goal-location-width: 0.0
  goal-location-height: 0.0

# ORCA related configurations. NOTE, this is named for human planning, but
# if does take the robot(s) states into consideration.
human-motion-planner:
  # TODO(alex): Details on safety space
  safety-space: 0.0
  # The default maximum distance (center point to center point) to other agents a
  # new agent takes into account in the navigation. The larger this number, the
  # longer the running time of the simulation. If the number is too low, the
  # simulation will not be safe. Must be non-negative.
  neighbor-distance: 10.0
  # The default maximum number of other agents a new agent takes into account in the
  # navigation. The larger this number, the longer the running time of the simulation.
  # If the number is too low, the simulation will not be safe.
  max-neighbors: 10
  # The default minimal amount of time for which a new agent's velocities that are
  # computed by the simulation are safe with respect to other agents. The larger this
  # number, the sooner an agent will respond to the presence of other agents, but the
  # less freedom the agent has in choosing its velocities. Must be positive.
  time-horizon: 5.0
  # The default minimal amount of time for which a new agent's velocities that are
  # computed by the simulation are safe with respect to obstacles. The larger this
  # number, the sooner an agent will respond to the presence of obstacles, but the
  # less freedom the agent has in choosing its velocities. Must be positive.
  time-horizon-obst: 5.0
  #The default radius of a new agent. Must be non-negative.
  radius: *human_radius
  kinematics: "holonomic"
  # The default maximum speed of a new agent. Must be non-negative
  max-speed: 1.0
  # The time step of the simulation. Must be positive.
  time-step: *time_step


# The various rewards and penalities for different behavors. The penalties are
# based on social forces.
incentives: &incentives_cfg
  # Reaching goal is rewarded
  success: 1.0
  # Collision is penalized
  collision: -0.25
  # Keep the robot from coming to close to other agents.
  discomfort-distance: 0.2
  # TODO(alex): What is this?
  discomfort-penalty-factor: 0.5

# Define the parameters used to generate and control human agents.
human: &human_cfg
  # Use Optimal Reciprocal Collision Avoidance to model humans.
  policy: "orca"
  # Radius of circular region around human. Used to determine collisions.
  radius: *human_radius
  # The velocity human wants to accelerate to reach.
  preferred-velocity: 1.0

# Define the parameters used to generate and control the robot.
robot: &robot_cfg
  # TODO(alex): Policy name?
  policy: None
  # Radius of circular region around robot. Used to determine collisions.
  radius: *robot_radius
  preferred-velocity: 1.0


policy:
  type: "PPO"

  # Define the various models used during training.
  models_cfg:
    # Social graph neural network.
    gnn-output-depth: &output-dim 32
    gnn: 
      # The dimension of the robot's state. The 9 values are:
      # [pos_x, pos_y, vel_x, vel_y, direction,
      # target_x, target_y, radius, preferred-velocity]
      robot-state-dimension: 4
      # The dimension of a human's observable state. The 5 values are:
      # pos_x, pos_y, vel_x, vel_y, direction.
      human-state-dimension: 5
      # The human and robot states are passed through a MLP to get equal output channels.
      # The last dimension here is used as the node channel depth in the graph network.
      mlp-layer-channels: [32, 32, *output-dim]
      # The number of layers in the graph model.
      gnn-layers: 6
    
    state-net:
      time-step: *time_step
      channel-depth: *output-dim

    value-net:
      channel-depth: *output-dim
      net-dims: [*output-dim, *output-dim, *output-dim, 1]

  robot_cfg: *robot_cfg
  human_cfg: *human_cfg
  incentive_cfg: *incentives_cfg